{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b3fefba",
   "metadata": {},
   "source": [
    "## Portfolio Assignment week 02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6833e723",
   "metadata": {},
   "source": [
    "### Text clustering "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebf096cc",
   "metadata": {},
   "source": [
    "Read, execute and analyse the code in the notebook tutorial_clustering_words. Then *choose one* of the assignments a), b) or c). \n",
    "\n",
    "a) read the article Clinical Documents Clustering Based on Medication/Symptom Names Using Multi-View Nonnegative Matrix Factorization. you can find the article <a href = 'https://pubmed.ncbi.nlm.nih.gov/26011887/'> here</a>. Explain the similarities of this notebook and the article. Explain in your own words what need to be added to this notebook to reproduce the article. There is no need to code the solution, you can mention in your own words the steps. \n",
    "\n",
    "b) Improve the outcome improving the data preprocessing and the hyper parameter configurations. Explain your choices. Your solution should be a coded solution with comments. Are there any other weighting solutions next to TF-IDF?\n",
    "\n",
    "c) Provide a text clustering solution with your own data of interest, you can follow a similar approach to the one in the tutorial_clustering_words notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61752f18",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8946d7cc",
   "metadata": {},
   "source": [
    "### b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96025ad9",
   "metadata": {},
   "source": [
    "\n",
    "**Data preprocessing in text data**\n",
    "1. **Tokenization**: \n",
    "    Break down the text into individual words or phrases, which will be used as features for the machine learning algorithm.\n",
    "2. **Stop Word Removal**: \n",
    "    Remove common words that do not carry much meaning, such as \"the\", \"and\", and \"a\".\n",
    "3. **Stemming/Lemmatization**: \n",
    "    Reduce words to their root form to avoid having multiple forms of the same word.\n",
    "4. **Normalization**: \n",
    "    Convert all text to lowercase to avoid having multiple forms of the same word.\n",
    "5. **Counting**: \n",
    "    Count the frequency of each word in the document and create a vector of word counts.\n",
    "\n",
    "**TF-IDF stands for Term Frequency-Inverse Document Frequency.**\n",
    "\n",
    "It is a numerical representation technique commonly used in natural language processing to evaluate the importance of words within a document relative to a collection of documents. The function of TF-IDF can be summarized as follows:\n",
    "\n",
    "1. **Term Frequency (TF):** TF measures the frequency of a word within a document. It gives higher weight to words that appear more often in the document, as they are likely to be more relevant to its content.\n",
    "\n",
    "2. **Inverse Document Frequency (IDF):** IDF assesses the uniqueness of a word by calculating how often it appears across the entire collection of documents. Words that appear in many documents receive lower IDF scores, while those that appear in fewer documents are assigned higher IDF scores.\n",
    "\n",
    "3. **Combining TF and IDF:** TF-IDF is computed by multiplying the TF of a word in a document by its IDF in the collection. This multiplication emphasizes words that are frequent within a document (high TF) but not so common across all documents (high IDF). This results in a numerical representation that reflects the importance of a word to a specific document within the context of the entire corpus.\n",
    "\n",
    "4. **Applications:** TF-IDF is commonly used for information retrieval, text mining, and document clustering. It helps in identifying keywords, classifying documents, and finding relevant documents in search engines.\n",
    "\n",
    "In essence, TF-IDF quantifies how relevant a word is to a document within the context of a larger collection. Words that are frequent in the document but rare in the collection receive higher TF-IDF scores, indicating their significance to the content of the document.\n",
    "\n",
    "**The Bag of Words (BoW) algorithm**\n",
    "\n",
    "it is a technique used in Natural Language Processing (NLP) for text modeling. It is a method of feature extraction that happens with text data. The BoW algorithm is used to preprocess the text by converting it into a bag of words, which keeps a count of the total occurrences of most frequently used words. The model is only concerned with whether known words occur in the document, not where in the document. The BoW algorithm is used to represent text data when modeling text with machine learning algorithms. The approach is very simple and flexible, and can be used in a myriad of ways for extracting features from documents.\n",
    "\n",
    "The BoW algorithm can be visualized using a table, which contains the count of words corresponding to the word itself. In practice, only a few words from the vocabulary, more preferably most common words are used to form the vector. The range of vocabulary is a big issue faced by the BoW model. For example, if the model comes across a new word it has not seen yet, rather we say a rare, but informative word like Biblioklept (means one who steals books), it will not be able to recognize it. Therefore, we select a particular number of most frequently used words. \n",
    "\n",
    "In conclusion, the BoW algorithm is a simple and effective way to represent text data for machine learning algorithms. It is widely used in NLP for tasks such as language modeling and document classification.\n",
    "\n",
    "**K-Means**\n",
    "\n",
    "K-Means is an algorithm that groups similar data points into clusters by iteratively adjusting cluster centers based on data point assignments. It's used for unsupervised clustering tasks like customer segmentation and data grouping.\n",
    "\n",
    "In conclusion,After upgrading the cleaning operation and improving the hyperparameters of the model, the classifications became closer in terms of meaning, and a better classification was done. The Bag of Words function is another algorithm used for feature extraction, and the clustering model was changed from MNF to Kmeans to evaluate the effectiveness of these factors in the model. Kmeans is classified based on distance. However, TF-IDF obtained a better answer because it selects better features based on logarithmic weighting algorithms compared to Bag of Words. The only feature selection criterion for Bag of Words is based on word count. The Bag of Words function is a natural language processing technique used to preprocess text data by converting it into a bag of words, which keeps a count of the total occurrences of most frequently used words. The Bag of Words model is a simple and flexible way of extracting features from documents. TF-IDF stands for term frequency-inverse document frequency and is a measure used in the fields of information retrieval and text mining that can quantify the importance or relevance of string representations in a document amongst a collection of documents.\n",
    "\n",
    "**Refrences:** \n",
    "1.https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html\n",
    "2.https://towardsdatascience.com/how-to-cluster-similar-sentences-using-tf-idf-and-graph-partitioning-in-python-5fb70627a4e0\n",
    "3.https://www.mygreatlearning.com/blog/bag-of-words/\n",
    "4.https://www.geeksforgeeks.org/k-means-clustering-introduction/\n",
    "\n",
    "**More imformation:**\n",
    "Shiva and i worked together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1616cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87fa9acc",
   "metadata": {},
   "source": [
    "Load text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "553dddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create an empty DataFrame called 'df' with columns 'docid' and 'text'\n",
    "df = pd.DataFrame(columns=['docid','text'])\n",
    "\n",
    "# Get all files ending with '.txt' in the 'data' directory\n",
    "docs = [x for x in glob.glob(\"data/*.txt\")]\n",
    "\n",
    "#Fill the DataFrame with data from the files\n",
    "for doc in docs:\n",
    "    txt = Path(doc).read_text(encoding=\"utf8\")\n",
    "    df.loc[len(df.index)] = [doc[:-4], txt]\n",
    "      \n",
    "df = df.set_index('docid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbfb7ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>docid</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data\\15939911</th>\n",
       "      <td>CASE: A 28-year-old previously healthy man pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\16778410</th>\n",
       "      <td>The patient was a 34-yr-old man who presented ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\17803823</th>\n",
       "      <td>A 23 year old white male with a 4 year history...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\18236639</th>\n",
       "      <td>A 30-year-old female (65 kg) underwent rhinopl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\18258107</th>\n",
       "      <td>Here, we describe another case in a 60-year-ol...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            text\n",
       "docid                                                           \n",
       "data\\15939911  CASE: A 28-year-old previously healthy man pre...\n",
       "data\\16778410  The patient was a 34-yr-old man who presented ...\n",
       "data\\17803823  A 23 year old white male with a 4 year history...\n",
       "data\\18236639  A 30-year-old female (65 kg) underwent rhinopl...\n",
       "data\\18258107  Here, we describe another case in a 60-year-ol..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8564d7a",
   "metadata": {},
   "source": [
    "## Cleaning the text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b80c1750",
   "metadata": {},
   "source": [
    "optimize the cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03f04d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove bad characters\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", text)\n",
    "    text = text.replace(\"#\",\"\").replace(\"\\u200c\",\" \").replace(\"/t\",\" \").replace(\"https:\",\"\")\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove square brackets and contents inside them\n",
    "    text = re.sub('\\[.*?\\]', ' ', text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "\n",
    "    # Remove alphanumeric words\n",
    "    text = re.sub('\\w*\\d\\w*', ' ', text)\n",
    "\n",
    "    # Remove special character '�'\n",
    "    text = re.sub('�', ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "cleaned = lambda x: clean_text(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82d3eaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noun extract and lemmatize function\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text \n",
    "    and pull out only the nouns.'''\n",
    "    # create mask to isolate words that are nouns\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    # store function to split string of words \n",
    "    # into a list of words (tokens)\n",
    "    tokenized = word_tokenize(text)\n",
    "    # store function to lemmatize each word\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    # use list comprehension to lemmatize all words \n",
    "    # and create a list of all nouns\n",
    "    all_nouns = [wordnet_lemmatizer.lemmatize(word) \\\n",
    "    for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    \n",
    "    #return string of joined list of nouns\n",
    "    return ' '.join(all_nouns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b0cb7e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>docid</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data\\15939911</th>\n",
       "      <td>case year man week history palpitation symptom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\16778410</th>\n",
       "      <td>patient man complaint fever cough smoker histo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\17803823</th>\n",
       "      <td>year male year history crohn disease day histo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\18236639</th>\n",
       "      <td>year female kg rhinoplasty anaesthesia combina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\18258107</th>\n",
       "      <td>case year man francisco pork philippine june m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            text\n",
       "docid                                                           \n",
       "data\\15939911  case year man week history palpitation symptom...\n",
       "data\\16778410  patient man complaint fever cough smoker histo...\n",
       "data\\17803823  year male year history crohn disease day histo...\n",
       "data\\18236639  year female kg rhinoplasty anaesthesia combina...\n",
       "data\\18258107  case year man francisco pork philippine june m..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean Text\n",
    "df[\"text\"] = df[\"text\"].apply(cleaned)\n",
    "data_nouns = pd.DataFrame(df[\"text\"].apply(nouns))\n",
    "# Visually Inspect\n",
    "data_nouns.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b29d7d37",
   "metadata": {},
   "source": [
    "improve stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1853612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stop words file\n",
    "SW_file = open(\"stop_words.txt\", \"r\", encoding=\"utf-8\")\n",
    "\n",
    "# Read line by line\n",
    "SW = SW_file.read().splitlines()\n",
    "\n",
    "# Close file\n",
    "SW_file.close()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0dcb7b77",
   "metadata": {},
   "source": [
    "Optimize the hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9bc7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abdomen</th>\n",
       "      <th>abdomen pelvis</th>\n",
       "      <th>ablation</th>\n",
       "      <th>abnormality</th>\n",
       "      <th>absence</th>\n",
       "      <th>accumulation</th>\n",
       "      <th>acid</th>\n",
       "      <th>acidosis</th>\n",
       "      <th>activity</th>\n",
       "      <th>acuity</th>\n",
       "      <th>...</th>\n",
       "      <th>year male</th>\n",
       "      <th>year man</th>\n",
       "      <th>year patient</th>\n",
       "      <th>year surgery</th>\n",
       "      <th>year treatment</th>\n",
       "      <th>year woman</th>\n",
       "      <th>zone</th>\n",
       "      <th>μg</th>\n",
       "      <th>μl</th>\n",
       "      <th>μmol</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>docid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data\\15939911</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.496285</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063863</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\16778410</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\17803823</th>\n",
       "      <td>0.080591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\18236639</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.334607</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\18258107</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127966</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                abdomen  abdomen pelvis  ablation  abnormality  absence  \\\n",
       "docid                                                                     \n",
       "data\\15939911  0.000000             0.0  0.496285          0.0      0.0   \n",
       "data\\16778410  0.000000             0.0  0.000000          0.0      0.0   \n",
       "data\\17803823  0.080591             0.0  0.000000          0.0      0.0   \n",
       "data\\18236639  0.000000             0.0  0.000000          0.0      0.0   \n",
       "data\\18258107  0.000000             0.0  0.000000          0.0      0.0   \n",
       "\n",
       "               accumulation  acid  acidosis  activity  acuity  ...  year male  \\\n",
       "docid                                                          ...              \n",
       "data\\15939911           0.0   0.0       0.0       0.0     0.0  ...   0.000000   \n",
       "data\\16778410           0.0   0.0       0.0       0.0     0.0  ...   0.000000   \n",
       "data\\17803823           0.0   0.0       0.0       0.0     0.0  ...   0.097003   \n",
       "data\\18236639           0.0   0.0       0.0       0.0     0.0  ...   0.000000   \n",
       "data\\18258107           0.0   0.0       0.0       0.0     0.0  ...   0.000000   \n",
       "\n",
       "               year man  year patient  year surgery  year treatment  \\\n",
       "docid                                                                 \n",
       "data\\15939911  0.063863           0.0           0.0             0.0   \n",
       "data\\16778410  0.000000           0.0           0.0             0.0   \n",
       "data\\17803823  0.000000           0.0           0.0             0.0   \n",
       "data\\18236639  0.000000           0.0           0.0             0.0   \n",
       "data\\18258107  0.065868           0.0           0.0             0.0   \n",
       "\n",
       "               year woman  zone        μg        μl  μmol  \n",
       "docid                                                      \n",
       "data\\15939911         0.0   0.0  0.000000  0.000000   0.0  \n",
       "data\\16778410         0.0   0.0  0.000000  0.000000   0.0  \n",
       "data\\17803823         0.0   0.0  0.000000  0.000000   0.0  \n",
       "data\\18236639         0.0   0.0  0.334607  0.000000   0.0  \n",
       "data\\18258107         0.0   0.0  0.000000  0.127966   0.0  \n",
       "\n",
       "[5 rows x 1106 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ngram_range let us to check the double words\n",
    "# min_df help to trim  not important words\n",
    "\n",
    "tv_noun = TfidfVectorizer(stop_words=Stopwords, ngram_range = (1,2), max_df = .8, min_df = 5)\n",
    "\n",
    "# Fit and Transform speech noun text to a TF-IDF Doc-Term Matrix\n",
    "data_tv_noun = tv_noun.fit_transform(data_nouns.text)\n",
    "# Create data-frame of Doc-Term Matrix with nouns as column names\n",
    "data_dtm_noun = pd.DataFrame(data_tv_noun.toarray(), columns=tv_noun.get_feature_names_out())\n",
    "data_dtm_noun.index = df.index\n",
    "# Visually inspect Document Term Matrix\n",
    "data_dtm_noun.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd41d4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, num_top_words, topic_names=None):\n",
    "    '''Given an NMF model, feature_names, and number of top words, print \n",
    "       topic number and its top feature names, up to specified number of top words.'''\n",
    "    # iterate through topics in topic-term matrix, 'H' aka\n",
    "    # model.components_\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        #print topic, topic number, and top words\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i] \\\n",
    "             for i in topic.argsort()[:-num_top_words - 1:-1]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5619fde",
   "metadata": {},
   "source": [
    "Optimize the number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5439cddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "heart, day, pressure, blood, hour, blood pressure, ejection fraction, ejection, function, ml, failure, rate, fraction, level, tachycardia\n",
      "\n",
      "Topic  1\n",
      "tumor, cell, lymph, lesion, tumor cell, node, lymph node, metastasis, fig, cm, nodule, mass, resection, ml, tomography\n",
      "\n",
      "Topic  2\n",
      "valve, echocardiography, leaflet, atrium, regurgitation, bypass, suture, tee, ventricle, ablation, artery, aorta, defect, murmur, failure\n",
      "\n",
      "Topic  3\n",
      "age, age year, parent, year age, muscle, month, gait, mri, seizure, brain, activity, child, level, gene, week\n",
      "\n",
      "Topic  4\n",
      "figure, cell, pain, vein, cm, carcinoma, examination, tumor, figure figure, malignancy, biopsy, muscle, wall, figure patient, sign\n",
      "\n",
      "Topic  5\n",
      "lung, day, chest, treatment, fig, culture, therapy, hospital, tuberculosis, respiratory, month, dyspnea, transplantation, effusion, sputum\n",
      "\n",
      "Topic  6\n",
      "mass, duct, cm, ct, tumour, fig, lesion, liver, carcinoma, examination, resection, abdomen, cyst, pain, wall\n",
      "\n",
      "Topic  7\n",
      "dl, mg, mg dl, level, platelet, count, blood, serum, ml, iu, day, aminotransferase, cell, antibody, μl\n",
      "\n",
      "Topic  8\n",
      "artery, balloon, graft, mm, stenosis, hematoma, mmhg, arm, flow, fig, left, angiogram, segment, catheter, site\n",
      "\n",
      "Topic  9\n",
      "rash, face, supplementation, deficiency, diagnosis, plaque, injection, presentation, skin, medication, nutrition, setting, dl, week, tube\n",
      "\n",
      "Topic  10\n",
      "eye, acuity, nerve, left, vision, injection, pupil, week, examination, lesion, field, day, image, hemorrhage, pain\n"
     ]
    }
   ],
   "source": [
    "nmf_model = NMF(11)\n",
    "# Learn an NMF model for given Document Term Matrix 'V' \n",
    "# Extract the document-topic matrix 'W'\n",
    "doc_topic = nmf_model.fit_transform(data_dtm_noun)\n",
    "# Extract top words from the topic-term matrix 'H' \n",
    "display_topics(nmf_model, tv_noun.get_feature_names_out(), 15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd1f920c",
   "metadata": {},
   "source": [
    "Bag of word is the other way to extract feature from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7933a819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abbott</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>abdominal</th>\n",
       "      <th>abdominis</th>\n",
       "      <th>abdomino</th>\n",
       "      <th>abdominopelvic</th>\n",
       "      <th>ablation</th>\n",
       "      <th>abnormal</th>\n",
       "      <th>abnormality</th>\n",
       "      <th>abr</th>\n",
       "      <th>...</th>\n",
       "      <th>µg</th>\n",
       "      <th>µl</th>\n",
       "      <th>µmol</th>\n",
       "      <th>µv</th>\n",
       "      <th>μg</th>\n",
       "      <th>μiu</th>\n",
       "      <th>μkat</th>\n",
       "      <th>μl</th>\n",
       "      <th>μm</th>\n",
       "      <th>μmol</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>docid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data\\15939911</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\16778410</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\17803823</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\18236639</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\18258107</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4318 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               abbott  abdomen  abdominal  abdominis  abdomino  \\\n",
       "docid                                                            \n",
       "data\\15939911       0        0          0          0         0   \n",
       "data\\16778410       0        0          0          0         0   \n",
       "data\\17803823       0        1          0          0         0   \n",
       "data\\18236639       0        0          0          0         0   \n",
       "data\\18258107       0        0          0          0         0   \n",
       "\n",
       "               abdominopelvic  ablation  abnormal  abnormality  abr  ...  µg  \\\n",
       "docid                                                                ...       \n",
       "data\\15939911               0         4         0            0    0  ...   0   \n",
       "data\\16778410               0         0         0            0    0  ...   0   \n",
       "data\\17803823               0         0         0            0    0  ...   0   \n",
       "data\\18236639               0         0         0            0    0  ...   0   \n",
       "data\\18258107               0         0         0            0    0  ...   0   \n",
       "\n",
       "               µl  µmol  µv  μg  μiu  μkat  μl  μm  μmol  \n",
       "docid                                                     \n",
       "data\\15939911   0     0   0   0    0     0   0   0     0  \n",
       "data\\16778410   0     0   0   0    0     0   0   0     0  \n",
       "data\\17803823   0     0   0   0    0     0   0   0     0  \n",
       "data\\18236639   0     0   0   3    0     0   0   0     0  \n",
       "data\\18258107   0     0   0   0    0     0   1   0     0  \n",
       "\n",
       "[5 rows x 4318 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "CountVec = CountVectorizer(ngram_range=(1,1), stop_words=Stopwords)\n",
    "\n",
    "data_tv_noun = CountVec.fit_transform(data_nouns.text)\n",
    "# Create data-frame of Doc-Term Matrix with nouns as column names\n",
    "data_dtm_noun = pd.DataFrame(data_tv_noun.toarray(), columns=CountVec.get_feature_names_out())\n",
    "data_dtm_noun.index = df.index\n",
    "# Visually inspect Document Term Matrix\n",
    "data_dtm_noun.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "072076fb",
   "metadata": {},
   "source": [
    "We enhanced the data cleaning process by implementing more comprehensive techniques and utilized an expanded stop words database. Additionally, we fine-tuned the parameters of the TF-IDF algorithm to achieve optimal performance. Furthermore, we optimized the number of clusters and employed the K-means++ clustering algorithm for improved accuracy. In addition to the TF-IDF algorithm, we also incorporated the Bag of Words algorithm for feature extraction. These enhancements collectively resulted in significant improvements to the overall analysis and classification process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113c032f",
   "metadata": {},
   "source": [
    "Clustring with K-means algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e70991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Specify the number of clusters\n",
    "num_clusters = 7\n",
    "\n",
    "# Create an instance of the KMeans clustering algorithm\n",
    "kmeans = KMeans(n_clusters=num_clusters, init=\"k-means++\")\n",
    "\n",
    "# Fit the KMeans algorithm to the data\n",
    "kmeans.fit(data_dtm_noun)\n",
    "\n",
    "# Get the cluster labels for each data point\n",
    "clusters = kmeans.labels_.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feab57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " rash, medication, face, supplementation, diagnosis, deficiency, injection, heart, platelet, drug, day, prednisolone, month, blood, week, history, neck, steroid, fistula, thrombocytopenia, count, skin, age year, chest, arm, figure, dl, mg, state, examination, presentation, autoimmune, test, dose, abdomen, improvement, echocardiogram, tube, hand, pain,\n",
      "\n",
      "Cluster 1:\n",
      " tumor, mass, cell, cm, figure, lesion, fig, examination, resection, lymph, ct, carcinoma, metastasis, tumor cell, month, nodule, node, biopsy, ml, surgery, dl, lymph node, diagnosis, level, size, tomography, history, specimen, cancer, day, duct, disease, abdomen, eu, wall, grade, invasion, diameter, tissue, calcium,\n",
      "\n",
      "Cluster 2:\n",
      " age, muscle, year age, parent, acidosis, birth, gene, child, analysis, liver, mri, lactate, seizure, acid, blood, age year, hour, pregnancy, section, care, month, activity, blocker, pressure, investigation, brain, blood pressure, matter, syndrome, cord, sequencing, variant, resuscitation, mmol, development, reference, limb, min, biopsy, weight,\n",
      "\n",
      "Cluster 3:\n",
      " valve, artery, vein, figure, balloon, leaflet, mmhg, operation, atrium, mm, echocardiography, day, catheter, month, bypass, tube, sign, regurgitation, surgery, graft, flow, fig, time, ablation, approach, tumour, stenosis, pressure, nerve, arm, seizure, ventricle, history, insertion, examination, suture, diameter, disease, sedation, heart,\n",
      "\n",
      "Cluster 4:\n",
      " fig, lung, mg, day, cell, treatment, blood, month, ct, dl, week, hospital, therapy, lesion, history, level, serum, chest, count, disease, examination, bone, chemotherapy, infection, diagnosis, left, range, platelet, admission, biopsy, test, respiratory, liver, analysis, symptom, response, tomography, mg dl, area, pain,\n",
      "\n",
      "Cluster 5:\n",
      " figure, day, heart, blood, level, pressure, function, ml, ejection fraction, ejection, echocardiography, blood pressure, tachycardia, fraction, hour, failure, pain, lead, heart failure, rate, wave, history, dyspnea, artery, examination, ecg, fibrillation, range, hospital, hypertension, mg, mm, treatment, lvef, ventricle, admission, μg, hg, extremity, home,\n",
      "\n",
      "Cluster 6:\n",
      " eye, acuity, nerve, day, figure, symptom, week, vision, colon, examination, atrophy, left, level, reflex, history, injection, loss, result, culture, movement, therapy, pressure, lesion, fluid, pupil, field, type, blood culture, blood, pain, headache, family history, count, area, tumor, limit, tenderness, arm, family, cell,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "\n",
    "# Get the indices that would sort the cluster centers\n",
    "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "# Get the feature names from the TfidfVectorizer\n",
    "terms = tv_noun.get_feature_names_out()\n",
    "\n",
    "# Iterate over each cluster\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    \n",
    "    # Iterate over the indices of the top terms in the current cluster\n",
    "    for ind in order_centroids[i, :40]:\n",
    "        print(' %s' % terms[ind], end=',')\n",
    "    \n",
    "    # Print a new line after printing the top terms for the current cluster\n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
