{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebf096cc",
   "metadata": {},
   "source": [
    "b) Improve the outcome improving the data preprocessing and the hyper parameter configurations. Explain your choices. Your solution should be a coded solution with comments. Are there any other weighting solutions next to TF-IDF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1616cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction import text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87fa9acc",
   "metadata": {},
   "source": [
    "Load text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "553dddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create an empty DataFrame called 'df' with columns 'docid' and 'text'\n",
    "df = pd.DataFrame(columns=['docid','text'])\n",
    "\n",
    "# Get all files ending with '.txt' in the 'data' directory\n",
    "docs = [x for x in glob.glob(\"data/*.txt\")]\n",
    "\n",
    "#Fill the DataFrame with data from the files\n",
    "for doc in docs:\n",
    "    txt = Path(doc).read_text(encoding=\"utf8\")\n",
    "    df.loc[len(df.index)] = [doc[:-4], txt]\n",
    "      \n",
    "df = df.set_index('docid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbfb7ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>docid</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data\\15939911</th>\n",
       "      <td>CASE: A 28-year-old previously healthy man pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\16778410</th>\n",
       "      <td>The patient was a 34-yr-old man who presented ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\17803823</th>\n",
       "      <td>A 23 year old white male with a 4 year history...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\18236639</th>\n",
       "      <td>A 30-year-old female (65 kg) underwent rhinopl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\18258107</th>\n",
       "      <td>Here, we describe another case in a 60-year-ol...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            text\n",
       "docid                                                           \n",
       "data\\15939911  CASE: A 28-year-old previously healthy man pre...\n",
       "data\\16778410  The patient was a 34-yr-old man who presented ...\n",
       "data\\17803823  A 23 year old white male with a 4 year history...\n",
       "data\\18236639  A 30-year-old female (65 kg) underwent rhinopl...\n",
       "data\\18258107  Here, we describe another case in a 60-year-ol..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8564d7a",
   "metadata": {},
   "source": [
    "## Cleaning the text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b80c1750",
   "metadata": {},
   "source": [
    "optimize the cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03f04d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove bad characters\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", text)\n",
    "    text = text.replace(\"#\",\"\").replace(\"\\u200c\",\" \").replace(\"/t\",\" \").replace(\"https:\",\"\")\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove square brackets and contents inside them\n",
    "    text = re.sub('\\[.*?\\]', ' ', text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "\n",
    "    # Remove alphanumeric words\n",
    "    text = re.sub('\\w*\\d\\w*', ' ', text)\n",
    "\n",
    "    # Remove special character '�'\n",
    "    text = re.sub('�', ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "cleaned = lambda x: clean_text(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82d3eaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noun extract and lemmatize function\n",
    "def nouns(text):\n",
    "    '''Given a string of text, tokenize the text \n",
    "    and pull out only the nouns.'''\n",
    "    # create mask to isolate words that are nouns\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    # store function to split string of words \n",
    "    # into a list of words (tokens)\n",
    "    tokenized = word_tokenize(text)\n",
    "    # store function to lemmatize each word\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    # use list comprehension to lemmatize all words \n",
    "    # and create a list of all nouns\n",
    "    all_nouns = [wordnet_lemmatizer.lemmatize(word) \\\n",
    "    for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
    "    \n",
    "    #return string of joined list of nouns\n",
    "    return ' '.join(all_nouns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b0cb7e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>docid</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data\\15939911</th>\n",
       "      <td>case year man week history palpitation symptom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\16778410</th>\n",
       "      <td>patient man complaint fever cough smoker histo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\17803823</th>\n",
       "      <td>year male year history crohn disease day histo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\18236639</th>\n",
       "      <td>year female kg rhinoplasty anaesthesia combina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\18258107</th>\n",
       "      <td>case year man francisco pork philippine june m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            text\n",
       "docid                                                           \n",
       "data\\15939911  case year man week history palpitation symptom...\n",
       "data\\16778410  patient man complaint fever cough smoker histo...\n",
       "data\\17803823  year male year history crohn disease day histo...\n",
       "data\\18236639  year female kg rhinoplasty anaesthesia combina...\n",
       "data\\18258107  case year man francisco pork philippine june m..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean Text\n",
    "df[\"text\"] = df[\"text\"].apply(cleaned)\n",
    "data_nouns = pd.DataFrame(df[\"text\"].apply(nouns))\n",
    "# Visually Inspect\n",
    "data_nouns.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b29d7d37",
   "metadata": {},
   "source": [
    "improve stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1853612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stop words file\n",
    "SW_file = open(\"stop_words.txt\", \"r\", encoding=\"utf-8\")\n",
    "\n",
    "# Read line by line\n",
    "SW = SW_file.read().splitlines()\n",
    "\n",
    "# Close file\n",
    "SW_file.close()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0dcb7b77",
   "metadata": {},
   "source": [
    "Optimize the hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0fec90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mehdi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['come', 'vis', 'viser', 'visest'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abdomen</th>\n",
       "      <th>abdomen pelvis</th>\n",
       "      <th>ablation</th>\n",
       "      <th>abnormality</th>\n",
       "      <th>absence</th>\n",
       "      <th>accumulation</th>\n",
       "      <th>acid</th>\n",
       "      <th>acidosis</th>\n",
       "      <th>activity</th>\n",
       "      <th>acuity</th>\n",
       "      <th>...</th>\n",
       "      <th>year male</th>\n",
       "      <th>year man</th>\n",
       "      <th>year patient</th>\n",
       "      <th>year surgery</th>\n",
       "      <th>year treatment</th>\n",
       "      <th>year woman</th>\n",
       "      <th>zone</th>\n",
       "      <th>μg</th>\n",
       "      <th>μl</th>\n",
       "      <th>μmol</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>docid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data\\15939911</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.496285</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063863</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\16778410</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\17803823</th>\n",
       "      <td>0.080591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\18236639</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.334607</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\18258107</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127966</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                abdomen  abdomen pelvis  ablation  abnormality  absence  \\\n",
       "docid                                                                     \n",
       "data\\15939911  0.000000             0.0  0.496285          0.0      0.0   \n",
       "data\\16778410  0.000000             0.0  0.000000          0.0      0.0   \n",
       "data\\17803823  0.080591             0.0  0.000000          0.0      0.0   \n",
       "data\\18236639  0.000000             0.0  0.000000          0.0      0.0   \n",
       "data\\18258107  0.000000             0.0  0.000000          0.0      0.0   \n",
       "\n",
       "               accumulation  acid  acidosis  activity  acuity  ...  year male  \\\n",
       "docid                                                          ...              \n",
       "data\\15939911           0.0   0.0       0.0       0.0     0.0  ...   0.000000   \n",
       "data\\16778410           0.0   0.0       0.0       0.0     0.0  ...   0.000000   \n",
       "data\\17803823           0.0   0.0       0.0       0.0     0.0  ...   0.097003   \n",
       "data\\18236639           0.0   0.0       0.0       0.0     0.0  ...   0.000000   \n",
       "data\\18258107           0.0   0.0       0.0       0.0     0.0  ...   0.000000   \n",
       "\n",
       "               year man  year patient  year surgery  year treatment  \\\n",
       "docid                                                                 \n",
       "data\\15939911  0.063863           0.0           0.0             0.0   \n",
       "data\\16778410  0.000000           0.0           0.0             0.0   \n",
       "data\\17803823  0.000000           0.0           0.0             0.0   \n",
       "data\\18236639  0.000000           0.0           0.0             0.0   \n",
       "data\\18258107  0.065868           0.0           0.0             0.0   \n",
       "\n",
       "               year woman  zone        μg        μl  μmol  \n",
       "docid                                                      \n",
       "data\\15939911         0.0   0.0  0.000000  0.000000   0.0  \n",
       "data\\16778410         0.0   0.0  0.000000  0.000000   0.0  \n",
       "data\\17803823         0.0   0.0  0.000000  0.000000   0.0  \n",
       "data\\18236639         0.0   0.0  0.334607  0.000000   0.0  \n",
       "data\\18258107         0.0   0.0  0.000000  0.127966   0.0  \n",
       "\n",
       "[5 rows x 1106 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ngram_range let us to check the double words\n",
    "# min_df help to trim  not important words\n",
    "\n",
    "tv_noun = TfidfVectorizer(stop_words=SW, ngram_range = (1,2), max_df = .8, min_df = 5)\n",
    "\n",
    "# Fit and Transform speech noun text to a TF-IDF Doc-Term Matrix\n",
    "data_tv_noun = tv_noun.fit_transform(data_nouns.text)\n",
    "# Create data-frame of Doc-Term Matrix with nouns as column names\n",
    "data_dtm_noun = pd.DataFrame(data_tv_noun.toarray(), columns=tv_noun.get_feature_names_out())\n",
    "data_dtm_noun.index = df.index\n",
    "# Visually inspect Document Term Matrix\n",
    "data_dtm_noun.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd41d4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, num_top_words, topic_names=None):\n",
    "    '''Given an NMF model, feature_names, and number of top words, print \n",
    "       topic number and its top feature names, up to specified number of top words.'''\n",
    "    # iterate through topics in topic-term matrix, 'H' aka\n",
    "    # model.components_\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        #print topic, topic number, and top words\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i] \\\n",
    "             for i in topic.argsort()[:-num_top_words - 1:-1]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5619fde",
   "metadata": {},
   "source": [
    "Optimize the number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5439cddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "heart, day, pressure, blood, hour, blood pressure, ejection fraction, ejection, function, ml, failure, rate, fraction, level, tachycardia\n",
      "\n",
      "Topic  1\n",
      "tumor, cell, lymph, lesion, tumor cell, node, lymph node, metastasis, fig, cm, nodule, mass, resection, ml, tomography\n",
      "\n",
      "Topic  2\n",
      "valve, echocardiography, leaflet, atrium, regurgitation, bypass, suture, tee, ventricle, ablation, artery, aorta, defect, murmur, failure\n",
      "\n",
      "Topic  3\n",
      "age, age year, parent, year age, muscle, month, gait, mri, seizure, brain, activity, child, level, gene, week\n",
      "\n",
      "Topic  4\n",
      "figure, cell, pain, vein, cm, carcinoma, examination, tumor, figure figure, malignancy, biopsy, muscle, wall, figure patient, sign\n",
      "\n",
      "Topic  5\n",
      "lung, day, chest, treatment, fig, culture, therapy, hospital, tuberculosis, respiratory, month, dyspnea, transplantation, effusion, sputum\n",
      "\n",
      "Topic  6\n",
      "mass, duct, cm, ct, tumour, fig, lesion, liver, carcinoma, examination, resection, abdomen, cyst, pain, wall\n",
      "\n",
      "Topic  7\n",
      "dl, mg, mg dl, level, platelet, count, blood, serum, ml, iu, day, aminotransferase, cell, antibody, μl\n",
      "\n",
      "Topic  8\n",
      "artery, balloon, graft, mm, stenosis, hematoma, mmhg, arm, flow, fig, left, angiogram, segment, catheter, site\n",
      "\n",
      "Topic  9\n",
      "rash, face, supplementation, deficiency, diagnosis, plaque, injection, presentation, skin, medication, nutrition, setting, dl, week, tube\n",
      "\n",
      "Topic  10\n",
      "eye, acuity, nerve, left, vision, injection, pupil, week, examination, lesion, field, day, image, hemorrhage, pain\n"
     ]
    }
   ],
   "source": [
    "nmf_model = NMF(11)\n",
    "# Learn an NMF model for given Document Term Matrix 'V' \n",
    "# Extract the document-topic matrix 'W'\n",
    "doc_topic = nmf_model.fit_transform(data_dtm_noun)\n",
    "# Extract top words from the topic-term matrix 'H' \n",
    "display_topics(nmf_model, tv_noun.get_feature_names_out(), 15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd1f920c",
   "metadata": {},
   "source": [
    "Bag of word is the other way to extract feature from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6bca1427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mehdi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['come', 'vis', 'viser', 'visest'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abbott</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>abdominal</th>\n",
       "      <th>abdominis</th>\n",
       "      <th>abdomino</th>\n",
       "      <th>abdominopelvic</th>\n",
       "      <th>ablation</th>\n",
       "      <th>abnormal</th>\n",
       "      <th>abnormality</th>\n",
       "      <th>abr</th>\n",
       "      <th>...</th>\n",
       "      <th>µg</th>\n",
       "      <th>µl</th>\n",
       "      <th>µmol</th>\n",
       "      <th>µv</th>\n",
       "      <th>μg</th>\n",
       "      <th>μiu</th>\n",
       "      <th>μkat</th>\n",
       "      <th>μl</th>\n",
       "      <th>μm</th>\n",
       "      <th>μmol</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>docid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data\\15939911</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\16778410</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\17803823</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\18236639</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data\\18258107</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4318 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               abbott  abdomen  abdominal  abdominis  abdomino  \\\n",
       "docid                                                            \n",
       "data\\15939911       0        0          0          0         0   \n",
       "data\\16778410       0        0          0          0         0   \n",
       "data\\17803823       0        1          0          0         0   \n",
       "data\\18236639       0        0          0          0         0   \n",
       "data\\18258107       0        0          0          0         0   \n",
       "\n",
       "               abdominopelvic  ablation  abnormal  abnormality  abr  ...  µg  \\\n",
       "docid                                                                ...       \n",
       "data\\15939911               0         4         0            0    0  ...   0   \n",
       "data\\16778410               0         0         0            0    0  ...   0   \n",
       "data\\17803823               0         0         0            0    0  ...   0   \n",
       "data\\18236639               0         0         0            0    0  ...   0   \n",
       "data\\18258107               0         0         0            0    0  ...   0   \n",
       "\n",
       "               µl  µmol  µv  μg  μiu  μkat  μl  μm  μmol  \n",
       "docid                                                     \n",
       "data\\15939911   0     0   0   0    0     0   0   0     0  \n",
       "data\\16778410   0     0   0   0    0     0   0   0     0  \n",
       "data\\17803823   0     0   0   0    0     0   0   0     0  \n",
       "data\\18236639   0     0   0   3    0     0   0   0     0  \n",
       "data\\18258107   0     0   0   0    0     0   1   0     0  \n",
       "\n",
       "[5 rows x 4318 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "CountVec = CountVectorizer(ngram_range=(1,1), stop_words=SW)\n",
    "\n",
    "data_tv_noun = CountVec.fit_transform(data_nouns.text)\n",
    "# Create data-frame of Doc-Term Matrix with nouns as column names\n",
    "data_dtm_noun = pd.DataFrame(data_tv_noun.toarray(), columns=CountVec.get_feature_names_out())\n",
    "data_dtm_noun.index = df.index\n",
    "# Visually inspect Document Term Matrix\n",
    "data_dtm_noun.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "072076fb",
   "metadata": {},
   "source": [
    "We enhanced the data cleaning process by implementing more comprehensive techniques and utilized an expanded stop words database. Additionally, we fine-tuned the parameters of the TF-IDF algorithm to achieve optimal performance. Furthermore, we optimized the number of clusters and employed the K-means++ clustering algorithm for improved accuracy. In addition to the TF-IDF algorithm, we also incorporated the Bag of Words algorithm for feature extraction. These enhancements collectively resulted in significant improvements to the overall analysis and classification process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
